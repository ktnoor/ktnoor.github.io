---
title: "Taxonomy-guided routing in capsule network for hierarchical image classification"
collection: publications
category: manuscripts
permalink: /publication/2024-ht-Capsnet
excerpt: 'This paper is about the consistency-aware hierarchical multi-label image classification using a deep capsule network.'
date: 2025-11-04
venue: 'Knowledge-Based Systems'
# slidesurl: 'http://ktnoor.github.io/files/slides1.pdf'
paperurl: 'http://ktnoor.github.io/files/ht-capsnet.pdf'
doiurl: 'https://doi.org/10.1016/j.knosys.2025.114444'
citation: '<i>K. T. Noor, W. Luo, A. Robles-Kelly, L. Y. Zhang, and M. R. Bouadjenek, “Taxonomy-guided routing in capsule network for hierarchical image classification,” Knowledge-Based Systems, vol. 329, p. 114444, Nov. 2025, doi: 10.1016/j.knosys.2025.114444.</i>'
---

Hierarchical multi-label classification in computer vision presents significant challenges in maintaining consistency across different levels of class granularity while capturing fine-grained visual details. This paper presents Taxonomy-aware Capsule Network (HT-CapsNet), a novel capsule network architecture that explicitly incorporates taxonomic relationships into its routing mechanism to address these challenges. Our key innovation lies in a taxonomy-aware routing algorithm that dynamically adjusts capsule connections based on known hierarchical relationships, enabling more effective learning of hierarchical features while enforcing taxonomic consistency. Extensive experiments on six benchmark datasets, including Fashion-MNIST, Marine-Tree, CIFAR-10, CIFAR-100, CUB-200-2011, and Stanford Cars, demonstrate that HT-CapsNet significantly outperforms existing methods across various hierarchical classification metrics. Notably, on CUB-200-2011, HT-CapsNet achieves absolute improvements of 10.32%, 10.2%, 10.3%, and 8.55% in hierarchical accuracy, F1-score, consistency, and exact match, respectively, compared to the best-performing baseline. On the Stanford Cars dataset, the model improves upon the best baseline by 21.69%, 18.29%, 37.34%, and 19.95% in the same metrics, demonstrating the robustness and effectiveness of our approach for complex hierarchical classification tasks.
